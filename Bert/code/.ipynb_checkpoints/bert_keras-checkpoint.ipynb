{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Model\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "    (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(input_tensor):\n",
    "    ndims = input_tensor.shape.ndims\n",
    "    if ndims < 2:\n",
    "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                     (input_tensor.shape))\n",
    "    if ndims == 2:\n",
    "        return input_tensor\n",
    "\n",
    "    width = input_tensor.shape[-1]\n",
    "    output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "    return output_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(seq_length=128,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "    \n",
    "    def transpose_for_scores(input_tensor, num_attention_heads, seq_length, size_per_head):\n",
    "        output_tensor = tf.reshape(input_tensor, [-1, seq_length, num_attention_heads, size_per_head])\n",
    "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "        return output_tensor\n",
    "        \n",
    "    \n",
    "    \n",
    "    # input_shape = [None, seq_length, hidden_size]\n",
    "    \n",
    "    inputs = tf.keras.Input([num_attention_heads*size_per_head])\n",
    "    reshape_inputs = reshape_matrix(inputs)\n",
    "    query_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=query_act)(reshape_inputs)\n",
    "    key_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=key_act)(reshape_inputs)\n",
    "    value_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=value_act)(reshape_inputs)\n",
    "    \n",
    "    query_layer = transpose_for_scores(query_layer, num_attention_heads, seq_length, size_per_head)\n",
    "    key_layer = transpose_for_scores(key_layer, num_attention_heads, seq_length, size_per_head)\n",
    "\n",
    "    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "    attention_scores = tf.multiply(attention_scores, 1.0/math.sqrt(float(size_per_head)))\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "        attention_scores += adder\n",
    "    \n",
    "    attention_probs = tf.keras.layers.Softmax()(attention_scores)\n",
    "    attention_probs = tf.keras.layers.Dropout(attention_probs_dropout_prob)(attention_probs)\n",
    "        \n",
    "    value_layer = transpose_for_scores(value_layer, num_attention_heads, seq_length, size_per_head)\n",
    "\n",
    "    context_layer = tf.matmul(attention_probs, value_layer)\n",
    "    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "    \n",
    "    if do_return_2d_tensor:\n",
    "        context_layer = tf.reshape(context_layer, [-1, num_attention_heads*size_per_head])\n",
    "    else:\n",
    "        context_layer = tf.reshape(context_layer, [-1, seq_length, num_attention_heads, size_per_head])\n",
    "    \n",
    "    return Model(inputs, context_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(seq_length=128,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "    inputs = tf.keras.Input([seq_length, hidden_size])\n",
    "    \n",
    "    if hidden_size % num_attention_heads != 0:\n",
    "        raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "    \n",
    "    attention_head_size = int(hidden_size / num_attention_heads)\n",
    "    prev_output = reshape_matrix(inputs)\n",
    "    \n",
    "    all_layer_outputs = []\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        layer_input = prev_output\n",
    "        \n",
    "        attention_heads = []\n",
    "        attention_head = attention_layer(seq_length=seq_length,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        num_attention_heads=num_attention_heads,\n",
    "                                        size_per_head=attention_head_size,\n",
    "                                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        do_return_2d_tensor=True)(layer_input)\n",
    "        attention_heads.append(attention_head)\n",
    "        \n",
    "        attention_output = None\n",
    "        \n",
    "        # attention layer를 한번에 연산할지 나눠서 연산할지에 따라 다르게 구성 => 논문에서는 두가지 방법은 사실상 같다고 언급\n",
    "        if len(attention_heads) == 1:\n",
    "            attention_output = attention_heads[0]\n",
    "        else:\n",
    "            attention_output = tf.concat(attention_heads, axis=-1)\n",
    "        \n",
    "        attention_output = tf.keras.layers.Dense(hidden_size)(attention_output)\n",
    "        attention_output = tf.keras.layers.Dropout(hidden_dropout_prob)(attention_output)\n",
    "        attention_output = tf.keras.layers.LayerNormalization()(attention_output + layer_input)\n",
    "        \n",
    "        intermediate_output = tf.keras.layers.Dense(intermediate_size, activation=intermediate_act_fn)(attention_output)\n",
    "        \n",
    "        layer_output = tf.keras.layers.Dense(hidden_size)(intermediate_output)\n",
    "        layer_output = tf.keras.layers.Dropout(hidden_dropout_prob)(layer_output)\n",
    "        layer_output = tf.keras.layers.LayerNormalization()(layer_output + attention_output)\n",
    "        \n",
    "        prev_output = layer_output\n",
    "        all_layer_outputs.append(layer_output)\n",
    "    \n",
    "    if do_return_all_layers:\n",
    "        final_outputs = []\n",
    "        for layer_output in all_layer_outputs:\n",
    "            final_output = tf.reshape(layer_output, [-1, seq_length, hidden_size])\n",
    "            final_outputs.append(final_output)\n",
    "        return Model(inputs, final_outputs)\n",
    "    else:\n",
    "        final_output = tf.reshape(prev_output, [-1, seq_length, hidden_size])\n",
    "        return Model(inputs, final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                input_mask=None,\n",
    "                token_type_ids=None,\n",
    "                use_one_hot_embedding=False):\n",
    "        \n",
    "        inputs = tf.keras.Input([config[\"seq_length\"]])\n",
    "        \n",
    "        config = config.copy()\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "        \n",
    "        if input_mask is None:\n",
    "            input_mask = tf.ones(shape=[None, inputs.shape[1]], dtype=tf.int32)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.zeros(shape=[inputs.shape[0], inputs.shape[1]], dtype=tf.int32)\n",
    "            \n",
    "        self.embedding_output = tf.keras.layers.Embedding(config[\"vocab_size\"], config[\"hidden_size\"])(inputs)\n",
    "        self.embedding_output = embedding_postprocessor(config[\"seq_length\"],\n",
    "                                                        config[\"hidden_size\"],\n",
    "                                                       use_token_type=True,\n",
    "                                                       token_type_ids=token_type_ids,\n",
    "                                                       token_type_vocab_size=config[\"type_vocab_size\"],\n",
    "                                                       token_type_embedding_name=\"token_type_embeddings\",\n",
    "                                                       use_position_embeddings=True,\n",
    "                                                       position_embedding_name=\"position_embeddings\",\n",
    "                                                       initializer_range=config[\"initializer_range\"],\n",
    "                                                       max_position_embeddings=config[\"max_position_embeddings\"],\n",
    "                                                       dropout_prob=config[\"hidden_dropout_prob\"])\n",
    "        \n",
    "        to_mask = tf.cast(tf.reshape(input_mask, [-1, 1, config[\"seq_length\"]]), tf.float32)\n",
    "        broadcast_ones = tf.ones(shape=[inputs.shape[0], inputs.shape[1], 1], dtype=tf.float32)\n",
    "        attention_mask = broadcast_ones * to_mask\n",
    "        \n",
    "        self.all_encoder_layer = transformer_model(seq_length = config[\"seq_length\"],\n",
    "                                                attention_mask=attention_mask,\n",
    "                                                hidden_size=config[\"hidden_size\"],\n",
    "                                                num_hidden_layers=config[\"num_hidden_layers\"],\n",
    "                                                num_attention_heads=config[\"num_attention_heads\"],\n",
    "                                                intermediate_size=config[\"intermediate_size\"],\n",
    "                                                intermediate_act_fn=get_activation(config[\"hidden_act\"]),\n",
    "                                                hidden_dropout_prob=config[\"hidden_dropout_prob\"],\n",
    "                                                attention_probs_dropout_prob=config[\"attention_probs_dropout_prob\"],\n",
    "                                                initializer_range=config[\"initializer_range\"],\n",
    "                                                do_return_all_layers=True)(self.embedding_output)\n",
    "        \n",
    "        self.sequence_output = self.all_encoder_layer[-1]\n",
    "        \n",
    "        self.model = Model(inputs, self.sequence_output)\n",
    "        \n",
    "        return self.model\n",
    "        \n",
    "    def get_pooled_output(self):\n",
    "        \n",
    "        first_token_tensor = tf.squeeze(self.model.output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.keras.layers.Dense(config[\"hidden_size\"],\n",
    "                                                  activation=tf.tanh)(first_token_tensor)\n",
    "\n",
    "        return self.pooled_output\n",
    "    \n",
    "    def get_sequence_output(self):\n",
    "        return self.model.output\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_postprocessor(seq_length,\n",
    "                            hidden_size,\n",
    "                            use_token_type=False,\n",
    "                            token_type_ids=None,\n",
    "                            token_type_vocab_size=16,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            use_position_embeddings=True,\n",
    "                            position_embedding_name=\"position_embeddings\",\n",
    "                            initializer_range=0.02,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1):\n",
    "\n",
    "    inputs = tf.keras.Input([seq_length, hidden_size])\n",
    "    \n",
    "    output = inputs\n",
    "    \n",
    "    if use_token_type:\n",
    "        if token_type_ids is None:\n",
    "            raise ValueError(\"'token_type_ids' must be specified if 'use_token_type' is True.\")\n",
    "        \n",
    "        token_type_table = tf.keras.layers.Embedding(token_type_vocab_size, inputs.shape[2])(inputs)\n",
    "        flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
    "        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "        token_type_embeddings = tf.reshape(token_type_embeddings, [-1, seq_length, inputs.shape[2]])\n",
    "        \n",
    "        output += token_type_embeddings\n",
    "        \n",
    "    if use_position_embeddings:\n",
    "        full_position_embeddings = tf.keras.layers.Embedding(max_position_embeddings, inputs.shape[2])\n",
    "        position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
    "        num_dims = len(output.shape.as_list())\n",
    "        \n",
    "        position_broadcast_shape = []\n",
    "        for _ in range(num_dim - 2):\n",
    "            position_broadcast_shape.append(1)\n",
    "        position_broadcast_shape.extend([seq_length, inputs.shape[2]])\n",
    "        position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
    "        \n",
    "        output += position_embeddings\n",
    "    \n",
    "    output = tf.keras.layers.LayerNormalization()(output)\n",
    "    output = tf.keras.layers.Dropout(dropout_prob)(output)\n",
    "    \n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "    \"vocab_size\" : 32000,\n",
    "    \"hidden_size\" : 768,\n",
    "    \"num_hidden_layers\" : 12,\n",
    "    \"num_attention_heads\" : 12,\n",
    "    \"intermediate_size\" : 3072,\n",
    "    \"hidden_act\" : 'gelu',\n",
    "    \"hidden_dropout_prob\" : 0.1,\n",
    "    \"attention_probs_dropout_prob\" : 0.1,\n",
    "    \"max_position_embeddings\" : 512,\n",
    "    \"type_vocab_size\" : 16,\n",
    "    \"initializer_range\" : 0.02,\n",
    "    \"seq_length\" : 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   3217\u001b[0m         shape = constant_op._tensor_shape_tensor_conversion_function(\n\u001b[0;32m-> 3218\u001b[0;31m             tensor_shape.TensorShape(shape))\n\u001b[0m\u001b[1;32m   3219\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    355\u001b[0m     raise ValueError(\n\u001b[0;32m--> 356\u001b[0;31m         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\n\u001b[0m\u001b[1;32m    357\u001b[0m   \u001b[0ms_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert a partially known TensorShape to a Tensor: (None, 128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-22875cb04aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ea11a05e0b0e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, is_training, input_mask, token_type_ids, use_one_hot_embedding)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0minput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   3219\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# Happens when shape is a list with tensor elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure it's a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "bert = BertModel(config=bert_config, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask_from_input_mask(seq_length, hidden_size, to_mask):\n",
    "    \n",
    "    inputs = tf.keras.Input([seq_length, hidden_size])\n",
    "    \n",
    "    to_mask = tf.cast(tf.reshape(to_mask, [-1, 1, seq_length]), tf.float32)\n",
    "    \n",
    "    broadcast_ones = tf.ones(shape=[inputs.shape[0], inputs.shape[1], 1], dtype=tf.float32)\n",
    "    \n",
    "    mask = broadcast_ones * to_mask\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function assert_greater_v2 in module tensorflow.python.ops.check_ops:\n",
      "\n",
      "assert_greater_v2(x, y, message=None, summarize=None, name=None)\n",
      "    Assert the condition `x > y` holds element-wise.\n",
      "    \n",
      "    This Op checks that `x[i] > y[i]` holds for every pair of (possibly\n",
      "    broadcast) elements of `x` and `y`. If both `x` and `y` are empty, this is\n",
      "    trivially satisfied.\n",
      "    \n",
      "    If `x` is not greater than `y` element-wise, `message`, as well as the first\n",
      "    `summarize` entries of `x` and `y` are printed, and `InvalidArgumentError` is\n",
      "    raised.\n",
      "    \n",
      "    Args:\n",
      "      x:  Numeric `Tensor`.\n",
      "      y:  Numeric `Tensor`, same dtype as and broadcastable to `x`.\n",
      "      message: A string to prefix to the default message.\n",
      "      summarize: Print this many entries of each tensor.\n",
      "      name: A name for this operation (optional).  Defaults to \"assert_greater\".\n",
      "    \n",
      "    Returns:\n",
      "      Op that raises `InvalidArgumentError` if `x > y` is False. This can be\n",
      "        used with `tf.control_dependencies` inside of `tf.function`s to block\n",
      "        followup computation until the check has executed.\n",
      "      @compatibility(eager)\n",
      "      returns None\n",
      "      @end_compatibility\n",
      "    \n",
      "    Raises:\n",
      "      InvalidArgumentError: if the check can be performed immediately and\n",
      "        `x > y` is False. The check can be performed immediately during eager\n",
      "        execution or if `x` and `y` are statically known.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.assert_greater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function control_dependencies in module tensorflow.python.framework.ops:\n",
      "\n",
      "control_dependencies(control_inputs)\n",
      "    Wrapper for `Graph.control_dependencies()` using the default graph.\n",
      "    \n",
      "    See `tf.Graph.control_dependencies` for more details.\n",
      "    \n",
      "    Note: *In TensorFlow 2 with eager and/or Autograph, you should not require\n",
      "    this method, as ops execute in the expected order thanks to automatic control\n",
      "    dependencies.* Only use `tf.control_dependencies` when working with v1\n",
      "    `tf.Graph` code.\n",
      "    \n",
      "    When eager execution is enabled, any callable object in the `control_inputs`\n",
      "    list will be called.\n",
      "    \n",
      "    Args:\n",
      "      control_inputs: A list of `Operation` or `Tensor` objects which must be\n",
      "        executed or computed before running the operations defined in the context.\n",
      "        Can also be `None` to clear the control dependencies. If eager execution\n",
      "        is enabled, any callable object in the `control_inputs` list will be\n",
      "        called.\n",
      "    \n",
      "    Returns:\n",
      "     A context manager that specifies control dependencies for all\n",
      "     operations constructed within the context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.control_dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24576000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32000 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
