{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Model\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "    (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(input_tensor):\n",
    "    ndims = input_tensor.shape.ndims\n",
    "    if ndims < 2:\n",
    "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                     (input_tensor.shape))\n",
    "    if ndims == 2:\n",
    "        return input_tensor\n",
    "\n",
    "    width = input_tensor.shape[-1]\n",
    "    output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "    return output_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(seq_length=128,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "    \n",
    "    def transpose_for_scores(input_tensor, num_attention_heads, seq_length, size_per_head):\n",
    "        output_tensor = tf.reshape(input_tensor, [-1, seq_length, num_attention_heads, size_per_head])\n",
    "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "        return output_tensor\n",
    "        \n",
    "    \n",
    "    \n",
    "    # input_shape = [None, seq_length, hidden_size]\n",
    "    \n",
    "    inputs = tf.keras.Input([num_attention_heads*size_per_head])\n",
    "    reshape_inputs = reshape_matrix(inputs)\n",
    "    query_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=query_act)(reshape_inputs)\n",
    "    key_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=key_act)(reshape_inputs)\n",
    "    value_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, activation=value_act)(reshape_inputs)\n",
    "    \n",
    "    query_layer = transpose_for_scores(query_layer, num_attention_heads, seq_length, size_per_head)\n",
    "    key_layer = transpose_for_scores(key_layer, num_attention_heads, seq_length, size_per_head)\n",
    "\n",
    "    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "    attention_scores = tf.multiply(attention_scores, 1.0/math.sqrt(float(size_per_head)))\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "        attention_scores += adder\n",
    "    \n",
    "    attention_probs = tf.keras.layers.Softmax()(attention_scores)\n",
    "    attention_probs = tf.keras.layers.Dropout(attention_probs_dropout_prob)(attention_probs)\n",
    "        \n",
    "    value_layer = transpose_for_scores(value_layer, num_attention_heads, seq_length, size_per_head)\n",
    "\n",
    "    context_layer = tf.matmul(attention_probs, value_layer)\n",
    "    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "    \n",
    "    if do_return_2d_tensor:\n",
    "        context_layer = tf.reshape(context_layer, [-1, num_attention_heads*size_per_head])\n",
    "    else:\n",
    "        context_layer = tf.reshape(context_layer, [-1, seq_length, num_attention_heads, size_per_head])\n",
    "    \n",
    "    return Model(inputs, context_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(seq_length=128,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "    inputs = tf.keras.Input([seq_length, hidden_size])\n",
    "    \n",
    "    if hidden_size % num_attention_heads != 0:\n",
    "        raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "    \n",
    "    attention_head_size = int(hidden_size / num_attention_heads)\n",
    "    prev_output = reshape_matrix(inputs)\n",
    "    \n",
    "    all_layer_outputs = []\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        layer_input = prev_output\n",
    "        \n",
    "        attention_heads = []\n",
    "        attention_head = attention_layer(\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        num_attention_heads=num_attention_heads,\n",
    "                                        size_per_head=attention_head_size,\n",
    "                                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        do_return_2d_tensor=True,\n",
    "                                        from_seq_length=seq_length,\n",
    "                                        to_seq_length=seq_length)(layer_input)\n",
    "        attention_heads.append(attention_head)\n",
    "        \n",
    "        attention_output = None\n",
    "        \n",
    "        # attention layer를 한번에 연산할지 나눠서 연산할지에 따라 다르게 구성 => 논문에서는 두가지 방법은 사실상 같다고 언급\n",
    "        if len(attention_heads) == 1:\n",
    "            attention_output = attention_heads[0]\n",
    "        else:\n",
    "            attention_output = tf.concat(attention_heads, axis=-1)\n",
    "        \n",
    "        attention_output = tf.keras.layers.Dense(hidden_size)(attention_output)\n",
    "        attention_output = tf.keras.layers.Dropout(hidden_dropout_prob)(attention_output)\n",
    "        attention_output = tf.keras.layers.LayerNormalization()(attention_output + layer_input)\n",
    "        \n",
    "        intermediate_output = tf.keras.layers.Dense(intermediate_size, activation=intermediate_act_fn)(attention_output)\n",
    "        \n",
    "        layer_output = tf.keras.layers.Dense(hidden_size)(intermediate_output)\n",
    "        layer_output = tf.keras.layers.Dropout(hidden_dropout_prob)(layer_output)\n",
    "        layer_output = tf.keras.layers.LayerNormalization()(layer_output + attention_output)\n",
    "        \n",
    "        prev_output = layer_output\n",
    "        all_layer_outputs.append(layer_output)\n",
    "    \n",
    "    if do_return_all_layers:\n",
    "        final_outputs = []\n",
    "        for layer_output in all_layer_outputs:\n",
    "            final_output = tf.reshape(layer_output, [-1, seq_length, hidden_size])\n",
    "            final_outputs.append(final_output)\n",
    "        return Model(inputs, final_outputs)\n",
    "    else:\n",
    "        final_output = tf.reshape(prev_output, [-1, seq_length, hidden_size])\n",
    "        return Model(inputs, final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding_postprocessor(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                seq_length,\n",
    "                hidden_size,\n",
    "                use_token_type=False,\n",
    "                token_type_ids=None,\n",
    "                token_type_vocab_size=16,\n",
    "                token_type_embedding_name=\"token_type_embeddings\",\n",
    "                use_position_embeddings=True,\n",
    "                position_embedding_name=\"position_embeddings\",\n",
    "                initializer_range=0.02,\n",
    "                max_position_embeddings=512,\n",
    "                dropout_prob=0.1):\n",
    "    \n",
    "        super(embedding_postprocessor, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_token_type = use_token_type\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.token_type_vocab_size = token_type_vocab_size\n",
    "        self.token_type_embedding_name = token_type_embedding_name\n",
    "        self.use_position_embeddings = use_position_embeddings\n",
    "        self.position_embedding_name = position_embedding_name\n",
    "        self.initializer_range = initializer_range\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(self.dropout_prob)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.token_embedding = self.add_weight(shape=(self.token_type_vocab_size, input_shape[-1]))\n",
    "        self.position_embedding = self.add_weight(shape=(self.max_position_embeddings, input_shape[-1]))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        output = inputs\n",
    "        \n",
    "        if self.use_token_type:\n",
    "            if self.token_type_ids is None:\n",
    "                raise ValueError(\"'token_type_ids' must be specified if 'use_token_type' is True.\")\n",
    "            \n",
    "            self.flat_token_type_ids = tf.reshape(self.token_type_ids, [-1])\n",
    "            one_hot_ids = tf.one_hot(flat_token_type_ids, depth=self.token_type_vocab_size)\n",
    "            token_type_embeddings = tf.matmul(one_hot_ids, self.token_embedding)\n",
    "            token_type_embeddings = tf.reshape(token_type_embeddings, inputs.shape)\n",
    "            \n",
    "            output += token_type_embeddings\n",
    "        \n",
    "        if self.use_position_embeddings:\n",
    "            position_embeddings = tf.slice(self.position_embedding, [0,0], [self.seq_length, -1])\n",
    "            \n",
    "            num_dims = len(output.shape)\n",
    "            \n",
    "            position_broadcast_shape = []\n",
    "            for _ in range(num_dims-2):\n",
    "                position_broadcast_shape.append(1)\n",
    "            position_broadcast_shape.extend([self.seq_length, inputs.shape[-1]])\n",
    "            position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
    "            \n",
    "            output += position_embeddings\n",
    "            \n",
    "        output = self.layer_norm(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                input_masks=None,\n",
    "                token_type_ids=None,\n",
    "                use_one_hot_embedding=False):\n",
    "        \n",
    "        super(BertModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        self.input_masks = input_masks\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.use_one_hot_embedding = use_one_hot_embedding\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.config[\"vocab_size\"], self.config[\"hidden_size\"])\n",
    "        self.embedding_postprocessor = embedding_postprocessor(config[\"seq_length\"],\n",
    "                                                        config[\"hidden_size\"],\n",
    "                                                       use_token_type=False,\n",
    "                                                       token_type_ids=token_type_ids,\n",
    "                                                       token_type_vocab_size=config[\"type_vocab_size\"],\n",
    "                                                       token_type_embedding_name=\"token_type_embeddings\",\n",
    "                                                       use_position_embeddings=True,\n",
    "                                                       position_embedding_name=\"position_embeddings\",\n",
    "                                                       initializer_range=config[\"initializer_range\"],\n",
    "                                                       max_position_embeddings=config[\"max_position_embeddings\"],\n",
    "                                                       dropout_prob=config[\"hidden_dropout_prob\"])\n",
    "        self.transformer_model = transformer_model(seq_length = config[\"seq_length\"],\n",
    "                                                attention_mask=None,\n",
    "                                                hidden_size=config[\"hidden_size\"],\n",
    "                                                num_hidden_layers=config[\"num_hidden_layers\"],\n",
    "                                                num_attention_heads=config[\"num_attention_heads\"],\n",
    "                                                intermediate_size=config[\"intermediate_size\"],\n",
    "                                                intermediate_act_fn='gelu',\n",
    "                                                hidden_dropout_prob=config[\"hidden_dropout_prob\"],\n",
    "                                                attention_probs_dropout_prob=config[\"attention_probs_dropout_prob\"],\n",
    "                                                initializer_range=config[\"initializer_range\"],\n",
    "                                                do_return_all_layers=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        config = self.config.copy()\n",
    "        if not self.is_training:\n",
    "            config[\"hidden_dropout_prob\"] = 0.0\n",
    "            config[\"attention_probs_dropout_prob\"] = 0.0\n",
    "        \n",
    "        if self.input_masks is None:\n",
    "            self.input_masks = tf.ones(shape=[inputs.shape[0], inputs.shape[1]], dtype=tf.int32)\n",
    "        \n",
    "        if self.token_type_ids is None:\n",
    "            self.token_type_ids = tf.zeros(shape=[inputs.shape[0], inputs.shape[1]], dtype=tf.int32)\n",
    "        \n",
    "        to_mask = tf.cast(tf.reshape(self.input_masks, [-1, 1, config[\"seq_length\"]]), tf.float32)\n",
    "        broadcast_ones = tf.ones(shape=[inputs.shape[0], inputs.shape[1], 1], dtype=tf.float32)\n",
    "        attention_mask = broadcast_ones * to_mask\n",
    "        \n",
    "        self.embedding_output = self.embedding(inputs)\n",
    "        self.embedding_output = self.embedding_postprocessor(self.embedding_output)\n",
    "        \n",
    "        self.transformer_model.attention_mask = attention_mask\n",
    "        self.all_encoder_layer = self.transformer_model(self.embedding_output)\n",
    "        \n",
    "        self.sequence_output = self.all_encoder_layer[-1]\n",
    "        \n",
    "        return self.sequence_output    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "    \"vocab_size\" : 32000,\n",
    "    \"hidden_size\" : 768,\n",
    "    \"num_hidden_layers\" : 12,\n",
    "    \"num_attention_heads\" : 12,\n",
    "    \"intermediate_size\" : 3072,\n",
    "    \"hidden_act\" : 'gelu',\n",
    "    \"hidden_dropout_prob\" : 0.1,\n",
    "    \"attention_probs_dropout_prob\" : 0.1,\n",
    "    \"max_position_embeddings\" : 512,\n",
    "    \"type_vocab_size\" : 16,\n",
    "    \"initializer_range\" : 0.02,\n",
    "    \"seq_length\" : 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel(config=bert_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer([128], batch_size=1),\n",
    "    bert\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert_model_1 (BertModel)     (1, 128, 768)             110037504 \n",
      "=================================================================\n",
      "Total params: 110,037,504\n",
      "Trainable params: 110,037,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
